---
title: "Practice Section 3 Part 2"
author: "Anurag Garg"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Library loading

```{r, echo=FALSE}
library(tidyverse)
library(dslabs)
library(rvest)
```


```{r}
# read raw murders data line by line
filename <- system.file("extdata/murders.csv", package = "dslabs")
lines <- readLines(filename)
lines %>% head()

# split at commas with str_split function, remove row of column names
x <- str_split(lines, ",") 
x %>% head()
col_names <- x[[1]]
x <- x[-1]

# extract first element of each list entry
library(purrr)
map(x, function(y) y[1]) %>% head()
map(x, 1) %>% head()

# extract columns 1-5 as characters, then convert to proper format - NOTE: DIFFERENT FROM VIDEO
dat <- data.frame(parse_guess(map_chr(x, 1)),
                  parse_guess(map_chr(x, 2)),
                  parse_guess(map_chr(x, 3)),
                  parse_guess(map_chr(x, 4)),
                  parse_guess(map_chr(x, 5))) %>%
  setNames(col_names)
  
dat %>% head
  
# more efficient code for the same thing
dat <- x %>%
  transpose() %>%
  map( ~ parse_guess(unlist(.))) %>%
  setNames(col_names) %>% 
  as.data.frame() 

# the simplify argument makes str_split return a matrix instead of a list
x <- str_split(lines, ",", simplify = TRUE) 
col_names <- x[1,]
x <- x[-1,]
x %>% as_data_frame() %>%
  setNames(col_names) %>%
  mutate_all(parse_guess)
```



## Case Study: Extracting a table from a PDF

One of the datasets provided in dslabs shows scientific funding rates by gender in the Netherlands:

```{r}
library(dslabs)
data("research_funding_rates")
research_funding_rates 
```


The data come from a paper External link published in the prestigious journal PNAS. However, the data are not provided in a spreadsheet; they are in a table in a PDF document. We could extract the numbers by hand, but this could lead to human error. Instead we can try to wrangle the data using R.
Downloading the data

We start by downloading the PDF document then importing it into R using the following code:

```{r}
library("pdftools")
temp_file <- tempfile()
url <- "https://www.pnas.org/action/downloadSupplement?doi=10.1073%2Fpnas.1510159112&file=pnas.201510159SI.pdf"
download.file(url, temp_file)
txt <- pdf_text(temp_file)
file.remove(temp_file)
```

If we examine the object text we notice that it is a character vector with an entry for each page. So we keep the page we want using the following code:

```{r}
raw_data_research_funding_rates <- txt[2]
```


The steps above can actually be skipped because we include the raw data in the dslabs package as well:

```{r}
data("raw_data_research_funding_rates")
```


Looking at the download

Examining this object,

```{r}
raw_data_research_funding_rates %>% head
```


we see that it is a long string. Each line on the page, including the table rows, is separated by the symbol for newline:

We can therefore can create a list with the lines of the text as elements:

```{r}
tab <- str_split(raw_data_research_funding_rates, "\n")
```


Because we start off with just one element in the string, we end up with a list with just one entry:

```{r}
tab <- tab[[1]]
```


By examining this object,

```{r}
tab %>% head
```


we see that the information for the column names is the third and fourth entires:

```{r}
the_names_1 <- tab[3]
the_names_2 <- tab[4]
```


In the table, the column information is spread across two lines. We want to create one vector with one name for each column. We can do this using some of the functions we have just learned.
Extracting the table data

Let's start with the first line:

```{r}
the_names_1
```


We want to remove the leading space and everything following the comma. We can use regex for the latter. Then we can obtain the elements by splitting using the space. We want to split only when there are 2 or more spaces to avoid splitting success rate. So we use the regex \\s{2,} as follows:

```{r}
the_names_1 <- the_names_1 %>%
  str_trim() %>%
  str_replace_all(",\\s.", "") %>%
  str_split("\\s{2,}", simplify = TRUE)
the_names_1
```


Now let's look at the second line:

```{r}
the_names_2
```


Here we want to trim the leading space and then split by space as we did for the first line:

```{r}
the_names_2 <- the_names_2 %>%
  str_trim() %>%
  str_split("\\s+", simplify = TRUE)
the_names_2
```


Now we can join these to generate one name for each column:

```{r}
tmp_names <- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = "_")
the_names <- c(the_names_2[1], tmp_names) %>%
  str_to_lower() %>%
  str_replace_all("\\s", "_")
the_names
```


Now we are ready to get the actual data. By examining the tab object, we notice that the information is in lines 6 through 14. We can use str_split() again to achieve our goal:

```{r}
new_research_funding_rates <- tab[6:14] %>%
  str_trim %>%
  str_split("\\s{2,}", simplify = TRUE) %>%
  data.frame(stringsAsFactors = FALSE) %>%
  setNames(the_names) %>%
  mutate_at(-1, parse_number)
new_research_funding_rates %>% head()
```


We can see that the objects are identical:

```{r}
identical(research_funding_rates, new_research_funding_rates)
```


### Recoder

```{r}
# life expectancy time series for Caribbean countries
library(dslabs)
data("gapminder")
gapminder %>% 
  filter(region=="Caribbean") %>%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()

# display long country names
gapminder %>% 
  filter(region=="Caribbean") %>%
  filter(str_length(country) >= 12) %>%
  distinct(country) 

# recode long country names and remake plot
gapminder %>% filter(region=="Caribbean") %>%
  mutate(country = recode(country, 
                          'Antigua and Barbuda'="Barbuda",
                          'Dominican Republic' = "DR",
                          'St. Vincent and the Grenadines' = "St. Vincent",
                          'Trinidad and Tobago' = "Trinidad")) %>%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()
```

